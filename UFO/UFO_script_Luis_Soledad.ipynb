{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsoledad\\Anaconda3\\envs\\pandas\\lib\\site-packages\\ipykernel\\ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\lsoledad\\Anaconda3\\envs\\pandas\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3427: DtypeWarning: Columns (5,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'data_dictionary.xlsx', 'learned_settings_city', 'UFO_script_Luis_Soledad.ipynb', 'UFO_sightings.csv', 'UFO_sightings_training.csv', \"['city']\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (commonFourGram, city), SimplePredicate: (sameThreeCharStartPredicate, city)), (SimplePredicate: (commonTwoTokens, city), TfidfNGramCanopyPredicate: (0.4, city)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ['city']\n",
      "Lectura en deduper exitosa\n",
      "clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.canopy_index:Removing stop word an\n",
      "INFO:dedupe.canopy_index:Removing stop word er\n",
      "INFO:dedupe.canopy_index:Removing stop word nd\n",
      "INFO:dedupe.canopy_index:Removing stop word be\n",
      "INFO:dedupe.canopy_index:Removing stop word en\n",
      "INFO:dedupe.canopy_index:Removing stop word is\n",
      "INFO:dedupe.canopy_index:Removing stop word ri\n",
      "INFO:dedupe.canopy_index:Removing stop word on\n",
      "INFO:dedupe.canopy_index:Removing stop word re\n",
      "INFO:dedupe.canopy_index:Removing stop word  c\n",
      "INFO:dedupe.canopy_index:Removing stop word co\n",
      "INFO:dedupe.canopy_index:Removing stop word ur\n",
      "INFO:dedupe.canopy_index:Removing stop word  a\n",
      "INFO:dedupe.canopy_index:Removing stop word al\n",
      "INFO:dedupe.canopy_index:Removing stop word ia\n",
      "INFO:dedupe.canopy_index:Removing stop word ng\n",
      "INFO:dedupe.canopy_index:Removing stop word ra\n",
      "INFO:dedupe.canopy_index:Removing stop word  s\n",
      "INFO:dedupe.canopy_index:Removing stop word es\n",
      "INFO:dedupe.canopy_index:Removing stop word in\n",
      "INFO:dedupe.canopy_index:Removing stop word s \n",
      "INFO:dedupe.canopy_index:Removing stop word  n\n",
      "INFO:dedupe.canopy_index:Removing stop word ch\n",
      "INFO:dedupe.canopy_index:Removing stop word ea\n",
      "INFO:dedupe.canopy_index:Removing stop word ke\n",
      "INFO:dedupe.canopy_index:Removing stop word  o\n",
      "INFO:dedupe.canopy_index:Removing stop word ll\n",
      "INFO:dedupe.canopy_index:Removing stop word ou\n",
      "INFO:dedupe.canopy_index:Removing stop word da\n",
      "INFO:dedupe.canopy_index:Removing stop word ro\n",
      "INFO:dedupe.canopy_index:Removing stop word hi\n",
      "INFO:dedupe.canopy_index:Removing stop word te\n",
      "INFO:dedupe.canopy_index:Removing stop word il\n",
      "INFO:dedupe.canopy_index:Removing stop word ta\n",
      "INFO:dedupe.canopy_index:Removing stop word  m\n",
      "INFO:dedupe.canopy_index:Removing stop word h \n",
      "INFO:dedupe.canopy_index:Removing stop word ne\n",
      "INFO:dedupe.canopy_index:Removing stop word t \n",
      "INFO:dedupe.canopy_index:Removing stop word la\n",
      "INFO:dedupe.canopy_index:Removing stop word n \n",
      "INFO:dedupe.canopy_index:Removing stop word de\n",
      "INFO:dedupe.canopy_index:Removing stop word ad\n",
      "INFO:dedupe.canopy_index:Removing stop word el\n",
      "INFO:dedupe.canopy_index:Removing stop word nt\n",
      "INFO:dedupe.canopy_index:Removing stop word ha\n",
      "INFO:dedupe.canopy_index:Removing stop word at\n",
      "INFO:dedupe.blocking:10000, 3.2666302 seconds\n",
      "INFO:dedupe.blocking:20000, 5.9258112 seconds\n",
      "INFO:dedupe.blocking:30000, 9.0956492 seconds\n",
      "INFO:dedupe.blocking:40000, 12.8232132 seconds\n",
      "INFO:dedupe.blocking:50000, 16.2095422 seconds\n",
      "INFO:dedupe.blocking:60000, 19.3671882 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casi no hay datos duplicados =D\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7fa138533b21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[0manalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreporte2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7fa138533b21>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[0mdf5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreporte2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A partir de aqu√≠ inicia el reporte:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[0manalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreporte2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import dedupe\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from causallift import CausalLift\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read UFO info\n",
    "def read_data():\n",
    "    data = pd.read_csv('C:/Users/lsoledad/Documents/UFO/UFO_sightings.csv')\n",
    "    path = os.listdir('C:/Users/lsoledad/Documents/UFO')\n",
    "    print(path)\n",
    "    return data,path\n",
    "def clean_data(data,path):\n",
    "    #########\n",
    "    data=data[1:]\n",
    "    #Drop exact duplicates ---80321 \n",
    "    Column=data.keys().tolist()\n",
    "    d= {'Column': ['' for x in range(len(data)+1)]}\n",
    "    df2 = pd.DataFrame(d)\n",
    "    for x in Column:\n",
    "        df2['Column'] = df2['Column'].astype(str).replace(' ','', regex=True) + data[x].fillna(' ').astype(str).replace(' ', '', regex=True)\n",
    "    \n",
    "    df3 = df2['Column'][df2['Column'].duplicated(keep=False) == False][1:]\n",
    "    df3 = data.loc[df3.index][Column]\n",
    "\n",
    "    \n",
    "    #Delete Na values in longitude,altitude, city and datetime columns.---66505\n",
    "    df3=df3[Column].dropna()\n",
    "\n",
    "    \n",
    "    #Drop exact duplicates of datetime|state|country|longitude |latitude : they are the keys to know where the UFO can be---63686\n",
    "    Columna=['datetime','state','country','longitude ','latitude']\n",
    "    d= {'Column': ['' for x in range(len(df3)+1)]}\n",
    "    df4 = pd.DataFrame(d)\n",
    "    for x in Columna:\n",
    "        df4['Column'] = df4['Column'].astype(str).replace(' ','', regex=True) + data[x].fillna(' ').astype(str).replace(' ', '', regex=True)\n",
    "    \n",
    "    df5 = df4['Column'][df4['Column'].duplicated(keep=False) == False][1:]\n",
    "    df5 = data.loc[df5.index][Column]\n",
    "    #Convert latitude and duration(seconds) to numeric and if one is not numeric, trucate it.---63683---63682\n",
    "    df5=df5[pd.to_numeric(df5['duration (seconds)'], errors='coerce').notnull()]\n",
    "    df5=df5[pd.to_numeric(df5['latitude'], errors='coerce').notnull()]\n",
    "    \n",
    "    df6=df5.loc[:, ['city']]\n",
    "\n",
    "\n",
    "    #Comienzo del algoritmo Machine Learning Dedupe\n",
    "    ##############################\n",
    "    df6 = df6.replace(np.nan,'None')   ##IMPORTANTISIMO, SI NO SE CORRE ESTO MEJOR QUE NO SE CORRA NADA\n",
    "    Column=['city']\n",
    "    data_d = {}\n",
    "    for x in range(len(df6)):\n",
    "        data_temp = {}\n",
    "        for column in Column:\n",
    "            data_temp[column] = str(df6.iloc[x][column]).strip()          \n",
    "        data_d[x] = data_temp\n",
    "\n",
    "    flag = True\n",
    "    settings_file = str(Column)\n",
    "    \n",
    "    if os.path.exists(settings_file):\n",
    "        flag = False\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as f:\n",
    "            deduper = dedupe.StaticDedupe(f)\n",
    "        print('Lectura en deduper exitosa')\n",
    "            \n",
    "        try:\n",
    "            reporte2 = clustering(data_d, df6, Column, deduper)\n",
    "            return df5,reporte2\n",
    "        except:\n",
    "            print('Casi no hay datos duplicados')\n",
    "            return df5\n",
    "        \n",
    "    if flag:\n",
    "        print('Empezando entrenamiento del modelo')\n",
    "        reporte2 = training_dedupe(data_d, df6, Column)\n",
    "        return df5,reporte2\n",
    "\n",
    "        return df5\n",
    "            \n",
    "\n",
    "#######################################\n",
    "\n",
    "def clustering(data_d, df, columns, deduper):\n",
    "    \n",
    "    \n",
    "    print('clustering...')\n",
    "    clustered_dupes = deduper.partition(data_d, 0.6)  #aqui habia 0.6\n",
    "    print('# duplicate sets', len(clustered_dupes))\n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "        for record_id, score in zip(records, scores):\n",
    "            cluster_membership[record_id] = {\"Cluster ID\": cluster_id,\n",
    "                                             \"confidence_score\": score\n",
    "                                            }\n",
    "    \n",
    "    result = pd.DataFrame.from_dict(cluster_membership).T.sort_index()\n",
    "    r = pd.merge(df, result, left_index=True, right_index=True)\n",
    "    r = r.sort_values(by=['Cluster ID'])\n",
    "    ids = r['Cluster ID']\n",
    "    r = r[ids.isin(ids[ids.duplicated()])]\n",
    "    \n",
    "    \n",
    "    ############### para el reporte#############\n",
    "    d = {'Valor': ['' for x in range(len(r))]}\n",
    "    reporte2 = pd.DataFrame(d)\n",
    "    for c in columns:\n",
    "        reporte2['Valor'] = reporte2['Valor'].astype(str) + r[c].reset_index(drop=True).fillna(' ').astype(str) + '|'\n",
    "        \n",
    "    \n",
    "    list_of_tuples = list(zip(reporte2['Valor'],\n",
    "                              r['Cluster ID'])) \n",
    "\n",
    "    result=pd.DataFrame(list_of_tuples, columns=['city','cluster'])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def training_dedupe(data_d, df, columns):\n",
    "    \n",
    "    #print(data_d)\n",
    "    settings_file = str(columns)\n",
    "    fields = []\n",
    "    for column in columns:\n",
    "        fields.append({'field' : column, 'type': 'String', 'has missing' : True})\n",
    "    \n",
    "    print(fields)    \n",
    "    # Create a new deduper object and pass our data model to it.\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "\n",
    "    deduper.prepare_training(data_d, blocked_proportion = 0.9)\n",
    "\n",
    "    pair = deduper.uncertain_pairs()\n",
    "    print(pair)\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    dedupe.console_label(deduper)\n",
    "\n",
    "    # Using the examples we just labeled, train the deduper and learn\n",
    "    # blocking predicates\n",
    "    deduper.train()\n",
    "\n",
    "    # Save our weights and predicates to disk.  If the settings file\n",
    "    # exists, we will skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    option = input('Deseas subir el nuevo archivo de entrenamiento a tu bucket? y/n')\n",
    "    if  'y' in option or 'Y' in option and 'n' not in option:\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "        \n",
    "        with open('learned_settings_city', \"w\") as f:\n",
    "            deduper.write_training(f)\n",
    "            print('Training file, loaded\\n\\n')\n",
    "            \n",
    "    return clustering(data_d, df, columns, deduper)\n",
    "\n",
    "\n",
    "def analysis(df5,reporte2):\n",
    "    print('This is a ML process that gives us the clusters for each city that can be similar into meaning or misspelled ')\n",
    "    print(reporte2)\n",
    "    state=df5['city'].groupby(df5['country']).value_counts().nlargest(5)\n",
    "    print('Top 5 Count of the cities group by country')\n",
    "    print(state)\n",
    "    plt.figure()\n",
    "    #state.plot.pie()\n",
    "    plot = state.plot.pie(y='city', figsize=(5, 5),autopct='%.2f')\n",
    "    print('The amount of times a UFO was seen by state and country')\n",
    "    cross=pd.crosstab(df5['country'], df5['state'], margins = True)\n",
    "    print(cross)\n",
    "    print('Most likely cities in the hole data that a UFO can be seen next time')\n",
    "    \n",
    "#Main function\n",
    "def main():\n",
    "    data,path=read_data()\n",
    "    df5,reporte2=clean_data(data,path)\n",
    "    print('Analysis starts here:')\n",
    "    analysis(df5,reporte2)\n",
    "    print('Doing an analysis of data results, we can say that is more likely that the next ocurrence could be in US')\n",
    "    print('Furthermore the main city is Seattle with latitude: 47.60621 longitude:-122.332071')\n",
    "    \n",
    "main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
